{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_PG_Models_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNBeMmfCtoqBDK++p3xBc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumi7/PG-StockManagement/blob/master/DL_PG_Models_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc5WWkL_K_Zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebc97d1b-2675-4664-ef7d-ddc47689c1f3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mB4z5QQO2lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"drive/My Drive/GROUP_OF_DATASETS/CHEMISTRY.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF8uAs33RhFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "e26dce56-fbb5-44e1-d335-434dd3a6ec0b"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Unnamed: 0.1', 'date', 'ishol/week', 'group', 'name',\n",
              "       'quantity', 'unit_cogs', 'monthly_Avgtemp', 'monthly_avg_FeelsLikeC',\n",
              "       'monthly_avg_HeatIndexC', 'monthly_avg_cloudcover',\n",
              "       'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVGzBihXSgXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "fd0c8b80-d024-4cc0-a219-7f76fc880f20"
      },
      "source": [
        "df['quantity'].describe()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2152.000000\n",
              "mean        4.530158\n",
              "std         9.761890\n",
              "min         0.200000\n",
              "25%         1.000000\n",
              "50%         2.000000\n",
              "75%         3.000000\n",
              "max       120.000000\n",
              "Name: quantity, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZaKGxFdSmyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "cbb4cdc0-b195-4100-c630-c36ff35741e1"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(df['quantity'])"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cef4e1fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B_oKYVGS7en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59799de2-dca2-4090-f4e4-6e7f39765e9d"
      },
      "source": [
        "df['quantity'].skew()"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.102735329529392"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJhRaaijS-QW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "580a9c75-b983-4894-de3b-8f3707aa8def"
      },
      "source": [
        "df['quantity'].kurt()"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47.641620762279814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfjt1h_YIpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(['Unnamed: 0','Unnamed: 0.1','group'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-3WYE4Sd1_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(['date'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZgn-XUIYio5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "93e8ec0e-352e-4a6b-8ead-7a37cc972a2c"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'name', 'quantity', 'unit_cogs', 'monthly_Avgtemp',\n",
              "       'monthly_avg_FeelsLikeC', 'monthly_avg_HeatIndexC',\n",
              "       'monthly_avg_cloudcover', 'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF0N-FXzeZ9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwgldOKvecDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "798f7ac3-87ff-496a-9e6c-b4f49d5e87cd"
      },
      "source": [
        "train"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ishol/week</th>\n",
              "      <th>name</th>\n",
              "      <th>quantity</th>\n",
              "      <th>unit_cogs</th>\n",
              "      <th>monthly_Avgtemp</th>\n",
              "      <th>monthly_avg_FeelsLikeC</th>\n",
              "      <th>monthly_avg_HeatIndexC</th>\n",
              "      <th>monthly_avg_cloudcover</th>\n",
              "      <th>monthly_avg_humidity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>9</td>\n",
              "      <td>SCIERKA UNIWERSALNA 5 SZT</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.06</td>\n",
              "      <td>17.065</td>\n",
              "      <td>17.10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>72.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>10</td>\n",
              "      <td>WORKI NA SMIECI 60 L 16 SZT</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.17</td>\n",
              "      <td>12.080</td>\n",
              "      <td>10.37</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>72.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>10</td>\n",
              "      <td>ZMYWAK KUCHENNY MALY 1 SZT</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.18</td>\n",
              "      <td>12.080</td>\n",
              "      <td>10.37</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>72.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1809</th>\n",
              "      <td>9</td>\n",
              "      <td>MYDLO BIALY JELEN 150 G</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.99</td>\n",
              "      <td>3.970</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>80.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>10</td>\n",
              "      <td>REX 300 G 3*KOLOR</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.09</td>\n",
              "      <td>15.835</td>\n",
              "      <td>14.73</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>69.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>9</td>\n",
              "      <td>TECZKA BIALA WIAZANA</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.165</td>\n",
              "      <td>-4.03</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>73.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1168</th>\n",
              "      <td>9</td>\n",
              "      <td>E DO PLUKANIA 0_9 L</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>19.920</td>\n",
              "      <td>20.06</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>69.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1950</th>\n",
              "      <td>12</td>\n",
              "      <td>BOND PIANKA DO GOLENIA 200 ML</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.47</td>\n",
              "      <td>0.370</td>\n",
              "      <td>-3.61</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>88.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>9</td>\n",
              "      <td>KRET GRANULKI 400 G</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.99</td>\n",
              "      <td>19.920</td>\n",
              "      <td>20.06</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>69.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>9</td>\n",
              "      <td>PLYN SILAN 1L SENSUAL</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.49</td>\n",
              "      <td>-0.420</td>\n",
              "      <td>-4.84</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>82.81</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1721 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ishol/week  ... monthly_avg_humidity\n",
              "895            9  ...                72.30\n",
              "572           10  ...                72.47\n",
              "583           10  ...                72.47\n",
              "1809           9  ...                80.60\n",
              "1477          10  ...                69.23\n",
              "...          ...  ...                  ...\n",
              "411            9  ...                73.23\n",
              "1168           9  ...                69.97\n",
              "1950          12  ...                88.58\n",
              "1193           9  ...                69.97\n",
              "99             9  ...                82.81\n",
              "\n",
              "[1721 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od-_Z_bHevCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target=train.quantity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXBda3xrTHTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "20c1c045-a18e-47da-cbcd-f788b0348f0b"
      },
      "source": [
        "corrmatrix= df.corr()\n",
        "print(corrmatrix)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        ishol/week  ...  monthly_avg_humidity\n",
            "ishol/week                1.000000  ...              0.453348\n",
            "quantity                 -0.001503  ...             -0.007148\n",
            "unit_cogs                 0.044564  ...              0.022351\n",
            "monthly_Avgtemp          -0.038260  ...             -0.670414\n",
            "monthly_avg_FeelsLikeC   -0.034126  ...             -0.664323\n",
            "monthly_avg_HeatIndexC    0.207298  ...             -0.462751\n",
            "monthly_avg_cloudcover    0.267031  ...              0.743852\n",
            "monthly_avg_humidity      0.453348  ...              1.000000\n",
            "\n",
            "[8 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fh7vq7uTd4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "9d9725ab-9895-4004-84b6-469a7367e164"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f,ax = plt.subplots(figsize=(12,9))\n",
        "sns.heatmap(corrmatrix, vmax=.8, square=True)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1cef536ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8B1rEDi0v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "257065f9-edac-4e88-a02c-805af8e1c541"
      },
      "source": [
        "print(train.shape)\n",
        "train=train.drop(['name'],axis=1)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1721, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1TvmKprynV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3270c86e-20b8-4e65-be37-685ed4575b15"
      },
      "source": [
        "train=train.drop(['quantity'],axis=1)\n",
        "train.shape"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1721, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_PGx6JCpoeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "922ce9ca-3677-4d57-ee3c-855b3bd00689"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1721, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ucEqkiprjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f58fbbd0-407b-4c50-cdf7-f73092893d89"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'unit_cogs', 'monthly_Avgtemp', 'monthly_avg_FeelsLikeC',\n",
              "       'monthly_avg_HeatIndexC', 'monthly_avg_cloudcover',\n",
              "       'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUw5LNKEXjOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TBgQEB_bsum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "055a0624-2746-447e-8183-f720850548ed"
      },
      "source": [
        "CNN_model = Sequential()\n",
        "CNN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
        "CNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "CNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "CNN_model.summary()"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 128)               1024      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 165,889\n",
            "Trainable params: 165,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nng4jtwddHaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCLmJCeHfdWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ae2d5681-7563-496a-d161-f56aaf536141"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'unit_cogs', 'monthly_Avgtemp', 'monthly_avg_FeelsLikeC',\n",
              "       'monthly_avg_HeatIndexC', 'monthly_avg_cloudcover',\n",
              "       'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_sVdbUYgh33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "1b3de565-e031-4240-9110-d6191946f0e7"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ishol/week                  int64\n",
              "unit_cogs                 float64\n",
              "monthly_Avgtemp           float64\n",
              "monthly_avg_FeelsLikeC    float64\n",
              "monthly_avg_HeatIndexC      int64\n",
              "monthly_avg_cloudcover      int64\n",
              "monthly_avg_humidity      float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1u1NV5DiewF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0eb29ccd-6ae0-4d5a-9bd7-a9c20eb63380"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05oR9pUsrVDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc46963c-d96c-47fa-a4e9-c635fc0366ce"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1721, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDl4WnNciiK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "864c49a8-e1eb-468b-9dae-d7df90352794"
      },
      "source": [
        "len(target)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpMMHT5BddNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "81279c08-b6d5-4718-a001-52b385acbe96"
      },
      "source": [
        "CNN_model.fit(train, target, epochs=500, batch_size=100, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1376 samples, validate on 345 samples\n",
            "Epoch 1/500\n",
            "1376/1376 [==============================] - 0s 201us/step - loss: 3.3729 - mean_absolute_error: 3.3729 - val_loss: 3.6885 - val_mean_absolute_error: 3.6885\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.68849, saving model to Weights-001--3.68849.hdf5\n",
            "Epoch 2/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.3125 - mean_absolute_error: 3.3125 - val_loss: 3.6964 - val_mean_absolute_error: 3.6964\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 3.68849\n",
            "Epoch 3/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.3388 - mean_absolute_error: 3.3388 - val_loss: 3.7203 - val_mean_absolute_error: 3.7203\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 3.68849\n",
            "Epoch 4/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.2913 - mean_absolute_error: 3.2913 - val_loss: 3.6951 - val_mean_absolute_error: 3.6951\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 3.68849\n",
            "Epoch 5/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.2714 - mean_absolute_error: 3.2714 - val_loss: 3.6621 - val_mean_absolute_error: 3.6621\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.68849 to 3.66210, saving model to Weights-005--3.66210.hdf5\n",
            "Epoch 6/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.2641 - mean_absolute_error: 3.2641 - val_loss: 3.6933 - val_mean_absolute_error: 3.6933\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 3.66210\n",
            "Epoch 7/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.2575 - mean_absolute_error: 3.2575 - val_loss: 3.6507 - val_mean_absolute_error: 3.6507\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.66210 to 3.65066, saving model to Weights-007--3.65066.hdf5\n",
            "Epoch 8/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 3.2321 - mean_absolute_error: 3.2321 - val_loss: 3.6457 - val_mean_absolute_error: 3.6457\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.65066 to 3.64572, saving model to Weights-008--3.64572.hdf5\n",
            "Epoch 9/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.2388 - mean_absolute_error: 3.2388 - val_loss: 3.6380 - val_mean_absolute_error: 3.6380\n",
            "\n",
            "Epoch 00009: val_loss improved from 3.64572 to 3.63799, saving model to Weights-009--3.63799.hdf5\n",
            "Epoch 10/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.2163 - mean_absolute_error: 3.2163 - val_loss: 3.6323 - val_mean_absolute_error: 3.6323\n",
            "\n",
            "Epoch 00010: val_loss improved from 3.63799 to 3.63231, saving model to Weights-010--3.63231.hdf5\n",
            "Epoch 11/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.2014 - mean_absolute_error: 3.2014 - val_loss: 3.6169 - val_mean_absolute_error: 3.6169\n",
            "\n",
            "Epoch 00011: val_loss improved from 3.63231 to 3.61695, saving model to Weights-011--3.61695.hdf5\n",
            "Epoch 12/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.2052 - mean_absolute_error: 3.2052 - val_loss: 3.6290 - val_mean_absolute_error: 3.6290\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 3.61695\n",
            "Epoch 13/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.2243 - mean_absolute_error: 3.2243 - val_loss: 3.6249 - val_mean_absolute_error: 3.6249\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 3.61695\n",
            "Epoch 14/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1917 - mean_absolute_error: 3.1917 - val_loss: 3.6074 - val_mean_absolute_error: 3.6074\n",
            "\n",
            "Epoch 00014: val_loss improved from 3.61695 to 3.60743, saving model to Weights-014--3.60743.hdf5\n",
            "Epoch 15/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1839 - mean_absolute_error: 3.1839 - val_loss: 3.6141 - val_mean_absolute_error: 3.6141\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 3.60743\n",
            "Epoch 16/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 3.1736 - mean_absolute_error: 3.1736 - val_loss: 3.5957 - val_mean_absolute_error: 3.5957\n",
            "\n",
            "Epoch 00016: val_loss improved from 3.60743 to 3.59573, saving model to Weights-016--3.59573.hdf5\n",
            "Epoch 17/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1620 - mean_absolute_error: 3.1620 - val_loss: 3.5965 - val_mean_absolute_error: 3.5965\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 3.59573\n",
            "Epoch 18/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1710 - mean_absolute_error: 3.1710 - val_loss: 3.6200 - val_mean_absolute_error: 3.6200\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 3.59573\n",
            "Epoch 19/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1957 - mean_absolute_error: 3.1957 - val_loss: 3.5915 - val_mean_absolute_error: 3.5915\n",
            "\n",
            "Epoch 00019: val_loss improved from 3.59573 to 3.59151, saving model to Weights-019--3.59151.hdf5\n",
            "Epoch 20/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1627 - mean_absolute_error: 3.1627 - val_loss: 3.5921 - val_mean_absolute_error: 3.5921\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 3.59151\n",
            "Epoch 21/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.1804 - mean_absolute_error: 3.1804 - val_loss: 3.6115 - val_mean_absolute_error: 3.6115\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 3.59151\n",
            "Epoch 22/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1739 - mean_absolute_error: 3.1739 - val_loss: 3.6016 - val_mean_absolute_error: 3.6016\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 3.59151\n",
            "Epoch 23/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1769 - mean_absolute_error: 3.1769 - val_loss: 3.5974 - val_mean_absolute_error: 3.5974\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 3.59151\n",
            "Epoch 24/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1547 - mean_absolute_error: 3.1547 - val_loss: 3.5731 - val_mean_absolute_error: 3.5731\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.59151 to 3.57309, saving model to Weights-024--3.57309.hdf5\n",
            "Epoch 25/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1679 - mean_absolute_error: 3.1679 - val_loss: 3.5634 - val_mean_absolute_error: 3.5634\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.57309 to 3.56343, saving model to Weights-025--3.56343.hdf5\n",
            "Epoch 26/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1286 - mean_absolute_error: 3.1286 - val_loss: 3.6405 - val_mean_absolute_error: 3.6405\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 3.56343\n",
            "Epoch 27/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.2064 - mean_absolute_error: 3.2064 - val_loss: 3.5732 - val_mean_absolute_error: 3.5732\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 3.56343\n",
            "Epoch 28/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1605 - mean_absolute_error: 3.1605 - val_loss: 3.5487 - val_mean_absolute_error: 3.5487\n",
            "\n",
            "Epoch 00028: val_loss improved from 3.56343 to 3.54873, saving model to Weights-028--3.54873.hdf5\n",
            "Epoch 29/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 3.1645 - mean_absolute_error: 3.1645 - val_loss: 3.5562 - val_mean_absolute_error: 3.5562\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 3.54873\n",
            "Epoch 30/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1276 - mean_absolute_error: 3.1276 - val_loss: 3.5513 - val_mean_absolute_error: 3.5513\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 3.54873\n",
            "Epoch 31/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 3.1471 - mean_absolute_error: 3.1471 - val_loss: 3.5321 - val_mean_absolute_error: 3.5321\n",
            "\n",
            "Epoch 00031: val_loss improved from 3.54873 to 3.53213, saving model to Weights-031--3.53213.hdf5\n",
            "Epoch 32/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 3.1378 - mean_absolute_error: 3.1378 - val_loss: 3.5388 - val_mean_absolute_error: 3.5388\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.53213\n",
            "Epoch 33/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1540 - mean_absolute_error: 3.1540 - val_loss: 3.6213 - val_mean_absolute_error: 3.6213\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.53213\n",
            "Epoch 34/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1937 - mean_absolute_error: 3.1937 - val_loss: 3.6111 - val_mean_absolute_error: 3.6111\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.53213\n",
            "Epoch 35/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1755 - mean_absolute_error: 3.1755 - val_loss: 3.5790 - val_mean_absolute_error: 3.5790\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.53213\n",
            "Epoch 36/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1669 - mean_absolute_error: 3.1669 - val_loss: 3.5482 - val_mean_absolute_error: 3.5482\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 3.53213\n",
            "Epoch 37/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1533 - mean_absolute_error: 3.1533 - val_loss: 3.5290 - val_mean_absolute_error: 3.5290\n",
            "\n",
            "Epoch 00037: val_loss improved from 3.53213 to 3.52898, saving model to Weights-037--3.52898.hdf5\n",
            "Epoch 38/500\n",
            "1376/1376 [==============================] - 0s 66us/step - loss: 3.1259 - mean_absolute_error: 3.1259 - val_loss: 3.5389 - val_mean_absolute_error: 3.5389\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.52898\n",
            "Epoch 39/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1240 - mean_absolute_error: 3.1240 - val_loss: 3.5513 - val_mean_absolute_error: 3.5513\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 3.52898\n",
            "Epoch 40/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1462 - mean_absolute_error: 3.1462 - val_loss: 3.5075 - val_mean_absolute_error: 3.5075\n",
            "\n",
            "Epoch 00040: val_loss improved from 3.52898 to 3.50747, saving model to Weights-040--3.50747.hdf5\n",
            "Epoch 41/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1380 - mean_absolute_error: 3.1380 - val_loss: 3.6406 - val_mean_absolute_error: 3.6406\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 3.50747\n",
            "Epoch 42/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1778 - mean_absolute_error: 3.1778 - val_loss: 3.5075 - val_mean_absolute_error: 3.5075\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 3.50747\n",
            "Epoch 43/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1489 - mean_absolute_error: 3.1489 - val_loss: 3.5109 - val_mean_absolute_error: 3.5109\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 3.50747\n",
            "Epoch 44/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1329 - mean_absolute_error: 3.1329 - val_loss: 3.4588 - val_mean_absolute_error: 3.4588\n",
            "\n",
            "Epoch 00044: val_loss improved from 3.50747 to 3.45881, saving model to Weights-044--3.45881.hdf5\n",
            "Epoch 45/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1314 - mean_absolute_error: 3.1314 - val_loss: 3.4511 - val_mean_absolute_error: 3.4511\n",
            "\n",
            "Epoch 00045: val_loss improved from 3.45881 to 3.45113, saving model to Weights-045--3.45113.hdf5\n",
            "Epoch 46/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1624 - mean_absolute_error: 3.1624 - val_loss: 3.5062 - val_mean_absolute_error: 3.5062\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 3.45113\n",
            "Epoch 47/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1266 - mean_absolute_error: 3.1266 - val_loss: 3.5548 - val_mean_absolute_error: 3.5548\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 3.45113\n",
            "Epoch 48/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1457 - mean_absolute_error: 3.1457 - val_loss: 3.4872 - val_mean_absolute_error: 3.4872\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 3.45113\n",
            "Epoch 49/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.1160 - mean_absolute_error: 3.1160 - val_loss: 3.4635 - val_mean_absolute_error: 3.4635\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 3.45113\n",
            "Epoch 50/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1166 - mean_absolute_error: 3.1166 - val_loss: 3.4960 - val_mean_absolute_error: 3.4960\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 3.45113\n",
            "Epoch 51/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.1455 - mean_absolute_error: 3.1455 - val_loss: 3.4226 - val_mean_absolute_error: 3.4226\n",
            "\n",
            "Epoch 00051: val_loss improved from 3.45113 to 3.42264, saving model to Weights-051--3.42264.hdf5\n",
            "Epoch 52/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1283 - mean_absolute_error: 3.1283 - val_loss: 3.4917 - val_mean_absolute_error: 3.4917\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 3.42264\n",
            "Epoch 53/500\n",
            "1376/1376 [==============================] - 0s 98us/step - loss: 3.0938 - mean_absolute_error: 3.0938 - val_loss: 3.4251 - val_mean_absolute_error: 3.4251\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 3.42264\n",
            "Epoch 54/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0918 - mean_absolute_error: 3.0918 - val_loss: 3.4522 - val_mean_absolute_error: 3.4522\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 3.42264\n",
            "Epoch 55/500\n",
            "1376/1376 [==============================] - 0s 89us/step - loss: 3.1398 - mean_absolute_error: 3.1398 - val_loss: 3.5652 - val_mean_absolute_error: 3.5652\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 3.42264\n",
            "Epoch 56/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1302 - mean_absolute_error: 3.1302 - val_loss: 3.3901 - val_mean_absolute_error: 3.3901\n",
            "\n",
            "Epoch 00056: val_loss improved from 3.42264 to 3.39006, saving model to Weights-056--3.39006.hdf5\n",
            "Epoch 57/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 3.0913 - mean_absolute_error: 3.0913 - val_loss: 3.4324 - val_mean_absolute_error: 3.4324\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 3.39006\n",
            "Epoch 58/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1031 - mean_absolute_error: 3.1031 - val_loss: 3.4125 - val_mean_absolute_error: 3.4125\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 3.39006\n",
            "Epoch 59/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1327 - mean_absolute_error: 3.1327 - val_loss: 3.3896 - val_mean_absolute_error: 3.3896\n",
            "\n",
            "Epoch 00059: val_loss improved from 3.39006 to 3.38963, saving model to Weights-059--3.38963.hdf5\n",
            "Epoch 60/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1153 - mean_absolute_error: 3.1153 - val_loss: 3.4126 - val_mean_absolute_error: 3.4126\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 3.38963\n",
            "Epoch 61/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0990 - mean_absolute_error: 3.0990 - val_loss: 3.3942 - val_mean_absolute_error: 3.3942\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 3.38963\n",
            "Epoch 62/500\n",
            "1376/1376 [==============================] - 0s 88us/step - loss: 3.1038 - mean_absolute_error: 3.1038 - val_loss: 3.3963 - val_mean_absolute_error: 3.3963\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 3.38963\n",
            "Epoch 63/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0746 - mean_absolute_error: 3.0746 - val_loss: 3.4122 - val_mean_absolute_error: 3.4122\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 3.38963\n",
            "Epoch 64/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1266 - mean_absolute_error: 3.1266 - val_loss: 3.4264 - val_mean_absolute_error: 3.4264\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 3.38963\n",
            "Epoch 65/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1371 - mean_absolute_error: 3.1371 - val_loss: 3.4555 - val_mean_absolute_error: 3.4555\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 3.38963\n",
            "Epoch 66/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0921 - mean_absolute_error: 3.0921 - val_loss: 3.5130 - val_mean_absolute_error: 3.5130\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 3.38963\n",
            "Epoch 67/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1187 - mean_absolute_error: 3.1187 - val_loss: 3.4552 - val_mean_absolute_error: 3.4552\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 3.38963\n",
            "Epoch 68/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1096 - mean_absolute_error: 3.1096 - val_loss: 3.4106 - val_mean_absolute_error: 3.4106\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 3.38963\n",
            "Epoch 69/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1231 - mean_absolute_error: 3.1231 - val_loss: 3.4933 - val_mean_absolute_error: 3.4933\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 3.38963\n",
            "Epoch 70/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.1438 - mean_absolute_error: 3.1438 - val_loss: 3.6187 - val_mean_absolute_error: 3.6187\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 3.38963\n",
            "Epoch 71/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.2276 - mean_absolute_error: 3.2276 - val_loss: 3.6205 - val_mean_absolute_error: 3.6205\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 3.38963\n",
            "Epoch 72/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.1802 - mean_absolute_error: 3.1802 - val_loss: 3.5462 - val_mean_absolute_error: 3.5462\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 3.38963\n",
            "Epoch 73/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1639 - mean_absolute_error: 3.1639 - val_loss: 3.5397 - val_mean_absolute_error: 3.5397\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 3.38963\n",
            "Epoch 74/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1423 - mean_absolute_error: 3.1423 - val_loss: 3.5177 - val_mean_absolute_error: 3.5177\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 3.38963\n",
            "Epoch 75/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1381 - mean_absolute_error: 3.1381 - val_loss: 3.4993 - val_mean_absolute_error: 3.4993\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 3.38963\n",
            "Epoch 76/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1308 - mean_absolute_error: 3.1308 - val_loss: 3.4845 - val_mean_absolute_error: 3.4845\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 3.38963\n",
            "Epoch 77/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1075 - mean_absolute_error: 3.1075 - val_loss: 3.5075 - val_mean_absolute_error: 3.5075\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 3.38963\n",
            "Epoch 78/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1260 - mean_absolute_error: 3.1260 - val_loss: 3.5015 - val_mean_absolute_error: 3.5015\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 3.38963\n",
            "Epoch 79/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0901 - mean_absolute_error: 3.0901 - val_loss: 3.4098 - val_mean_absolute_error: 3.4098\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 3.38963\n",
            "Epoch 80/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1191 - mean_absolute_error: 3.1191 - val_loss: 3.4352 - val_mean_absolute_error: 3.4352\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 3.38963\n",
            "Epoch 81/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1421 - mean_absolute_error: 3.1421 - val_loss: 3.5352 - val_mean_absolute_error: 3.5352\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 3.38963\n",
            "Epoch 82/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0803 - mean_absolute_error: 3.0803 - val_loss: 3.3570 - val_mean_absolute_error: 3.3570\n",
            "\n",
            "Epoch 00082: val_loss improved from 3.38963 to 3.35695, saving model to Weights-082--3.35695.hdf5\n",
            "Epoch 83/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0845 - mean_absolute_error: 3.0845 - val_loss: 3.4546 - val_mean_absolute_error: 3.4546\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 3.35695\n",
            "Epoch 84/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0656 - mean_absolute_error: 3.0656 - val_loss: 3.3636 - val_mean_absolute_error: 3.3636\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 3.35695\n",
            "Epoch 85/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0661 - mean_absolute_error: 3.0661 - val_loss: 3.3257 - val_mean_absolute_error: 3.3257\n",
            "\n",
            "Epoch 00085: val_loss improved from 3.35695 to 3.32571, saving model to Weights-085--3.32571.hdf5\n",
            "Epoch 86/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.1000 - mean_absolute_error: 3.1000 - val_loss: 3.5188 - val_mean_absolute_error: 3.5188\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 3.32571\n",
            "Epoch 87/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.0703 - mean_absolute_error: 3.0703 - val_loss: 3.3108 - val_mean_absolute_error: 3.3108\n",
            "\n",
            "Epoch 00087: val_loss improved from 3.32571 to 3.31082, saving model to Weights-087--3.31082.hdf5\n",
            "Epoch 88/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1608 - mean_absolute_error: 3.1608 - val_loss: 3.4908 - val_mean_absolute_error: 3.4908\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 3.31082\n",
            "Epoch 89/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0965 - mean_absolute_error: 3.0965 - val_loss: 3.3810 - val_mean_absolute_error: 3.3810\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 3.31082\n",
            "Epoch 90/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0807 - mean_absolute_error: 3.0807 - val_loss: 3.5595 - val_mean_absolute_error: 3.5595\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 3.31082\n",
            "Epoch 91/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 3.1141 - mean_absolute_error: 3.1141 - val_loss: 3.3223 - val_mean_absolute_error: 3.3223\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 3.31082\n",
            "Epoch 92/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1637 - mean_absolute_error: 3.1637 - val_loss: 3.6257 - val_mean_absolute_error: 3.6257\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 3.31082\n",
            "Epoch 93/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.2269 - mean_absolute_error: 3.2269 - val_loss: 3.6224 - val_mean_absolute_error: 3.6224\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 3.31082\n",
            "Epoch 94/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.1815 - mean_absolute_error: 3.1815 - val_loss: 3.5870 - val_mean_absolute_error: 3.5870\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 3.31082\n",
            "Epoch 95/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1617 - mean_absolute_error: 3.1617 - val_loss: 3.5751 - val_mean_absolute_error: 3.5751\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 3.31082\n",
            "Epoch 96/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.1411 - mean_absolute_error: 3.1411 - val_loss: 3.5752 - val_mean_absolute_error: 3.5752\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 3.31082\n",
            "Epoch 97/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.1657 - mean_absolute_error: 3.1657 - val_loss: 3.5628 - val_mean_absolute_error: 3.5628\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 3.31082\n",
            "Epoch 98/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1365 - mean_absolute_error: 3.1365 - val_loss: 3.5538 - val_mean_absolute_error: 3.5538\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 3.31082\n",
            "Epoch 99/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1151 - mean_absolute_error: 3.1151 - val_loss: 3.5011 - val_mean_absolute_error: 3.5011\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 3.31082\n",
            "Epoch 100/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1082 - mean_absolute_error: 3.1082 - val_loss: 3.4894 - val_mean_absolute_error: 3.4894\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 3.31082\n",
            "Epoch 101/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.1178 - mean_absolute_error: 3.1178 - val_loss: 3.4926 - val_mean_absolute_error: 3.4926\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 3.31082\n",
            "Epoch 102/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1237 - mean_absolute_error: 3.1237 - val_loss: 3.5014 - val_mean_absolute_error: 3.5014\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 3.31082\n",
            "Epoch 103/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1122 - mean_absolute_error: 3.1122 - val_loss: 3.4112 - val_mean_absolute_error: 3.4112\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 3.31082\n",
            "Epoch 104/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0841 - mean_absolute_error: 3.0841 - val_loss: 3.3727 - val_mean_absolute_error: 3.3727\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 3.31082\n",
            "Epoch 105/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0617 - mean_absolute_error: 3.0617 - val_loss: 3.3768 - val_mean_absolute_error: 3.3768\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 3.31082\n",
            "Epoch 106/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0789 - mean_absolute_error: 3.0789 - val_loss: 3.4219 - val_mean_absolute_error: 3.4219\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 3.31082\n",
            "Epoch 107/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0479 - mean_absolute_error: 3.0479 - val_loss: 3.3146 - val_mean_absolute_error: 3.3146\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 3.31082\n",
            "Epoch 108/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0721 - mean_absolute_error: 3.0721 - val_loss: 3.3021 - val_mean_absolute_error: 3.3021\n",
            "\n",
            "Epoch 00108: val_loss improved from 3.31082 to 3.30210, saving model to Weights-108--3.30210.hdf5\n",
            "Epoch 109/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0618 - mean_absolute_error: 3.0618 - val_loss: 3.4216 - val_mean_absolute_error: 3.4216\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 3.30210\n",
            "Epoch 110/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0299 - mean_absolute_error: 3.0299 - val_loss: 3.3142 - val_mean_absolute_error: 3.3142\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 3.30210\n",
            "Epoch 111/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0628 - mean_absolute_error: 3.0628 - val_loss: 3.5580 - val_mean_absolute_error: 3.5580\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 3.30210\n",
            "Epoch 112/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1507 - mean_absolute_error: 3.1507 - val_loss: 3.5429 - val_mean_absolute_error: 3.5429\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 3.30210\n",
            "Epoch 113/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1105 - mean_absolute_error: 3.1105 - val_loss: 3.4706 - val_mean_absolute_error: 3.4706\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 3.30210\n",
            "Epoch 114/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0835 - mean_absolute_error: 3.0835 - val_loss: 3.4009 - val_mean_absolute_error: 3.4009\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 3.30210\n",
            "Epoch 115/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0658 - mean_absolute_error: 3.0658 - val_loss: 3.3105 - val_mean_absolute_error: 3.3105\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 3.30210\n",
            "Epoch 116/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1452 - mean_absolute_error: 3.1452 - val_loss: 3.5467 - val_mean_absolute_error: 3.5467\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 3.30210\n",
            "Epoch 117/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.1210 - mean_absolute_error: 3.1210 - val_loss: 3.3724 - val_mean_absolute_error: 3.3724\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 3.30210\n",
            "Epoch 118/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0481 - mean_absolute_error: 3.0481 - val_loss: 3.3158 - val_mean_absolute_error: 3.3158\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 3.30210\n",
            "Epoch 119/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0486 - mean_absolute_error: 3.0486 - val_loss: 3.3538 - val_mean_absolute_error: 3.3538\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 3.30210\n",
            "Epoch 120/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0277 - mean_absolute_error: 3.0277 - val_loss: 3.3264 - val_mean_absolute_error: 3.3264\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 3.30210\n",
            "Epoch 121/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0630 - mean_absolute_error: 3.0630 - val_loss: 3.3778 - val_mean_absolute_error: 3.3778\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 3.30210\n",
            "Epoch 122/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0300 - mean_absolute_error: 3.0300 - val_loss: 3.3222 - val_mean_absolute_error: 3.3222\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 3.30210\n",
            "Epoch 123/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.1026 - mean_absolute_error: 3.1026 - val_loss: 3.5312 - val_mean_absolute_error: 3.5312\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 3.30210\n",
            "Epoch 124/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 3.1071 - mean_absolute_error: 3.1071 - val_loss: 3.3832 - val_mean_absolute_error: 3.3832\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 3.30210\n",
            "Epoch 125/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0863 - mean_absolute_error: 3.0863 - val_loss: 3.4584 - val_mean_absolute_error: 3.4584\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 3.30210\n",
            "Epoch 126/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0627 - mean_absolute_error: 3.0627 - val_loss: 3.4898 - val_mean_absolute_error: 3.4898\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 3.30210\n",
            "Epoch 127/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0989 - mean_absolute_error: 3.0989 - val_loss: 3.3618 - val_mean_absolute_error: 3.3618\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 3.30210\n",
            "Epoch 128/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0316 - mean_absolute_error: 3.0316 - val_loss: 3.3481 - val_mean_absolute_error: 3.3481\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 3.30210\n",
            "Epoch 129/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0287 - mean_absolute_error: 3.0287 - val_loss: 3.3248 - val_mean_absolute_error: 3.3248\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 3.30210\n",
            "Epoch 130/500\n",
            "1376/1376 [==============================] - 0s 84us/step - loss: 3.0909 - mean_absolute_error: 3.0909 - val_loss: 3.4764 - val_mean_absolute_error: 3.4764\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 3.30210\n",
            "Epoch 131/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0794 - mean_absolute_error: 3.0794 - val_loss: 3.3631 - val_mean_absolute_error: 3.3631\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 3.30210\n",
            "Epoch 132/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 3.0547 - mean_absolute_error: 3.0547 - val_loss: 3.4102 - val_mean_absolute_error: 3.4102\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 3.30210\n",
            "Epoch 133/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0544 - mean_absolute_error: 3.0544 - val_loss: 3.3050 - val_mean_absolute_error: 3.3050\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 3.30210\n",
            "Epoch 134/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0374 - mean_absolute_error: 3.0374 - val_loss: 3.3532 - val_mean_absolute_error: 3.3532\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 3.30210\n",
            "Epoch 135/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0039 - mean_absolute_error: 3.0039 - val_loss: 3.3404 - val_mean_absolute_error: 3.3404\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 3.30210\n",
            "Epoch 136/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0645 - mean_absolute_error: 3.0645 - val_loss: 3.2906 - val_mean_absolute_error: 3.2906\n",
            "\n",
            "Epoch 00136: val_loss improved from 3.30210 to 3.29061, saving model to Weights-136--3.29061.hdf5\n",
            "Epoch 137/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1151 - mean_absolute_error: 3.1151 - val_loss: 3.3632 - val_mean_absolute_error: 3.3632\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 3.29061\n",
            "Epoch 138/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0694 - mean_absolute_error: 3.0694 - val_loss: 3.3484 - val_mean_absolute_error: 3.3484\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 3.29061\n",
            "Epoch 139/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0348 - mean_absolute_error: 3.0348 - val_loss: 3.5095 - val_mean_absolute_error: 3.5095\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 3.29061\n",
            "Epoch 140/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1709 - mean_absolute_error: 3.1709 - val_loss: 3.5706 - val_mean_absolute_error: 3.5706\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 3.29061\n",
            "Epoch 141/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1153 - mean_absolute_error: 3.1153 - val_loss: 3.3998 - val_mean_absolute_error: 3.3998\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 3.29061\n",
            "Epoch 142/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0550 - mean_absolute_error: 3.0550 - val_loss: 3.3150 - val_mean_absolute_error: 3.3150\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 3.29061\n",
            "Epoch 143/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0444 - mean_absolute_error: 3.0444 - val_loss: 3.3020 - val_mean_absolute_error: 3.3020\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 3.29061\n",
            "Epoch 144/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0149 - mean_absolute_error: 3.0149 - val_loss: 3.3104 - val_mean_absolute_error: 3.3104\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 3.29061\n",
            "Epoch 145/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0276 - mean_absolute_error: 3.0276 - val_loss: 3.2964 - val_mean_absolute_error: 3.2964\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 3.29061\n",
            "Epoch 146/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 3.0272 - mean_absolute_error: 3.0272 - val_loss: 3.3248 - val_mean_absolute_error: 3.3248\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 3.29061\n",
            "Epoch 147/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0317 - mean_absolute_error: 3.0317 - val_loss: 3.4124 - val_mean_absolute_error: 3.4124\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 3.29061\n",
            "Epoch 148/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0210 - mean_absolute_error: 3.0210 - val_loss: 3.4026 - val_mean_absolute_error: 3.4026\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 3.29061\n",
            "Epoch 149/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0452 - mean_absolute_error: 3.0452 - val_loss: 3.2945 - val_mean_absolute_error: 3.2945\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 3.29061\n",
            "Epoch 150/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0723 - mean_absolute_error: 3.0723 - val_loss: 3.3429 - val_mean_absolute_error: 3.3429\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 3.29061\n",
            "Epoch 151/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0848 - mean_absolute_error: 3.0848 - val_loss: 3.4833 - val_mean_absolute_error: 3.4833\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 3.29061\n",
            "Epoch 152/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.0482 - mean_absolute_error: 3.0482 - val_loss: 3.3331 - val_mean_absolute_error: 3.3331\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 3.29061\n",
            "Epoch 153/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0129 - mean_absolute_error: 3.0129 - val_loss: 3.3317 - val_mean_absolute_error: 3.3317\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 3.29061\n",
            "Epoch 154/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0361 - mean_absolute_error: 3.0361 - val_loss: 3.3178 - val_mean_absolute_error: 3.3178\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 3.29061\n",
            "Epoch 155/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0640 - mean_absolute_error: 3.0640 - val_loss: 3.3552 - val_mean_absolute_error: 3.3552\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 3.29061\n",
            "Epoch 156/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0489 - mean_absolute_error: 3.0489 - val_loss: 3.4094 - val_mean_absolute_error: 3.4094\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 3.29061\n",
            "Epoch 157/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0667 - mean_absolute_error: 3.0667 - val_loss: 3.3301 - val_mean_absolute_error: 3.3301\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 3.29061\n",
            "Epoch 158/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.1129 - mean_absolute_error: 3.1129 - val_loss: 3.5167 - val_mean_absolute_error: 3.5167\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 3.29061\n",
            "Epoch 159/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 3.0724 - mean_absolute_error: 3.0724 - val_loss: 3.2960 - val_mean_absolute_error: 3.2960\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 3.29061\n",
            "Epoch 160/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0295 - mean_absolute_error: 3.0295 - val_loss: 3.3334 - val_mean_absolute_error: 3.3334\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 3.29061\n",
            "Epoch 161/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 3.0295 - mean_absolute_error: 3.0295 - val_loss: 3.4431 - val_mean_absolute_error: 3.4431\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 3.29061\n",
            "Epoch 162/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0485 - mean_absolute_error: 3.0485 - val_loss: 3.2924 - val_mean_absolute_error: 3.2924\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 3.29061\n",
            "Epoch 163/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0231 - mean_absolute_error: 3.0231 - val_loss: 3.2917 - val_mean_absolute_error: 3.2917\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 3.29061\n",
            "Epoch 164/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0341 - mean_absolute_error: 3.0341 - val_loss: 3.3302 - val_mean_absolute_error: 3.3302\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 3.29061\n",
            "Epoch 165/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0548 - mean_absolute_error: 3.0548 - val_loss: 3.2889 - val_mean_absolute_error: 3.2889\n",
            "\n",
            "Epoch 00165: val_loss improved from 3.29061 to 3.28885, saving model to Weights-165--3.28885.hdf5\n",
            "Epoch 166/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0206 - mean_absolute_error: 3.0206 - val_loss: 3.3245 - val_mean_absolute_error: 3.3245\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 3.28885\n",
            "Epoch 167/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0222 - mean_absolute_error: 3.0222 - val_loss: 3.4359 - val_mean_absolute_error: 3.4359\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 3.28885\n",
            "Epoch 168/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 3.0308 - mean_absolute_error: 3.0308 - val_loss: 3.3624 - val_mean_absolute_error: 3.3624\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 3.28885\n",
            "Epoch 169/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0333 - mean_absolute_error: 3.0333 - val_loss: 3.3128 - val_mean_absolute_error: 3.3128\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 3.28885\n",
            "Epoch 170/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.1371 - mean_absolute_error: 3.1371 - val_loss: 3.5453 - val_mean_absolute_error: 3.5453\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 3.28885\n",
            "Epoch 171/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0902 - mean_absolute_error: 3.0902 - val_loss: 3.3769 - val_mean_absolute_error: 3.3769\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 3.28885\n",
            "Epoch 172/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0100 - mean_absolute_error: 3.0100 - val_loss: 3.3376 - val_mean_absolute_error: 3.3376\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 3.28885\n",
            "Epoch 173/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0104 - mean_absolute_error: 3.0104 - val_loss: 3.3255 - val_mean_absolute_error: 3.3255\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 3.28885\n",
            "Epoch 174/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9947 - mean_absolute_error: 2.9947 - val_loss: 3.3671 - val_mean_absolute_error: 3.3671\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 3.28885\n",
            "Epoch 175/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0045 - mean_absolute_error: 3.0045 - val_loss: 3.2851 - val_mean_absolute_error: 3.2851\n",
            "\n",
            "Epoch 00175: val_loss improved from 3.28885 to 3.28515, saving model to Weights-175--3.28515.hdf5\n",
            "Epoch 176/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1157 - mean_absolute_error: 3.1157 - val_loss: 3.6526 - val_mean_absolute_error: 3.6526\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 3.28515\n",
            "Epoch 177/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1988 - mean_absolute_error: 3.1988 - val_loss: 3.6407 - val_mean_absolute_error: 3.6407\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 3.28515\n",
            "Epoch 178/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.1760 - mean_absolute_error: 3.1760 - val_loss: 3.5612 - val_mean_absolute_error: 3.5612\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 3.28515\n",
            "Epoch 179/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.1351 - mean_absolute_error: 3.1351 - val_loss: 3.5417 - val_mean_absolute_error: 3.5417\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 3.28515\n",
            "Epoch 180/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.1134 - mean_absolute_error: 3.1134 - val_loss: 3.5219 - val_mean_absolute_error: 3.5219\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 3.28515\n",
            "Epoch 181/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0891 - mean_absolute_error: 3.0891 - val_loss: 3.4740 - val_mean_absolute_error: 3.4740\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 3.28515\n",
            "Epoch 182/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0762 - mean_absolute_error: 3.0762 - val_loss: 3.4742 - val_mean_absolute_error: 3.4742\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 3.28515\n",
            "Epoch 183/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0641 - mean_absolute_error: 3.0641 - val_loss: 3.4628 - val_mean_absolute_error: 3.4628\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 3.28515\n",
            "Epoch 184/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0649 - mean_absolute_error: 3.0649 - val_loss: 3.4432 - val_mean_absolute_error: 3.4432\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 3.28515\n",
            "Epoch 185/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0691 - mean_absolute_error: 3.0691 - val_loss: 3.4323 - val_mean_absolute_error: 3.4323\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 3.28515\n",
            "Epoch 186/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0724 - mean_absolute_error: 3.0724 - val_loss: 3.4304 - val_mean_absolute_error: 3.4304\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 3.28515\n",
            "Epoch 187/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0575 - mean_absolute_error: 3.0575 - val_loss: 3.4291 - val_mean_absolute_error: 3.4291\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 3.28515\n",
            "Epoch 188/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0465 - mean_absolute_error: 3.0465 - val_loss: 3.4248 - val_mean_absolute_error: 3.4248\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 3.28515\n",
            "Epoch 189/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0585 - mean_absolute_error: 3.0585 - val_loss: 3.4270 - val_mean_absolute_error: 3.4270\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 3.28515\n",
            "Epoch 190/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0684 - mean_absolute_error: 3.0684 - val_loss: 3.4158 - val_mean_absolute_error: 3.4158\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 3.28515\n",
            "Epoch 191/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0509 - mean_absolute_error: 3.0509 - val_loss: 3.4245 - val_mean_absolute_error: 3.4245\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 3.28515\n",
            "Epoch 192/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0855 - mean_absolute_error: 3.0856 - val_loss: 3.4126 - val_mean_absolute_error: 3.4126\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 3.28515\n",
            "Epoch 193/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 3.0435 - mean_absolute_error: 3.0435 - val_loss: 3.3816 - val_mean_absolute_error: 3.3816\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 3.28515\n",
            "Epoch 194/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0486 - mean_absolute_error: 3.0486 - val_loss: 3.3787 - val_mean_absolute_error: 3.3787\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 3.28515\n",
            "Epoch 195/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0297 - mean_absolute_error: 3.0297 - val_loss: 3.3925 - val_mean_absolute_error: 3.3925\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 3.28515\n",
            "Epoch 196/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0353 - mean_absolute_error: 3.0353 - val_loss: 3.3734 - val_mean_absolute_error: 3.3734\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 3.28515\n",
            "Epoch 197/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0491 - mean_absolute_error: 3.0491 - val_loss: 3.3208 - val_mean_absolute_error: 3.3208\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 3.28515\n",
            "Epoch 198/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 3.0678 - mean_absolute_error: 3.0678 - val_loss: 3.4164 - val_mean_absolute_error: 3.4164\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 3.28515\n",
            "Epoch 199/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0791 - mean_absolute_error: 3.0791 - val_loss: 3.3590 - val_mean_absolute_error: 3.3590\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 3.28515\n",
            "Epoch 200/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 3.0438 - mean_absolute_error: 3.0438 - val_loss: 3.3646 - val_mean_absolute_error: 3.3646\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 3.28515\n",
            "Epoch 201/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0580 - mean_absolute_error: 3.0580 - val_loss: 3.4582 - val_mean_absolute_error: 3.4582\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 3.28515\n",
            "Epoch 202/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 3.0785 - mean_absolute_error: 3.0785 - val_loss: 3.3139 - val_mean_absolute_error: 3.3139\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 3.28515\n",
            "Epoch 203/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0512 - mean_absolute_error: 3.0512 - val_loss: 3.3607 - val_mean_absolute_error: 3.3607\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 3.28515\n",
            "Epoch 204/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0488 - mean_absolute_error: 3.0488 - val_loss: 3.3771 - val_mean_absolute_error: 3.3771\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 3.28515\n",
            "Epoch 205/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0232 - mean_absolute_error: 3.0232 - val_loss: 3.3411 - val_mean_absolute_error: 3.3411\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 3.28515\n",
            "Epoch 206/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0482 - mean_absolute_error: 3.0482 - val_loss: 3.4065 - val_mean_absolute_error: 3.4065\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 3.28515\n",
            "Epoch 207/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0458 - mean_absolute_error: 3.0458 - val_loss: 3.3540 - val_mean_absolute_error: 3.3540\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 3.28515\n",
            "Epoch 208/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0315 - mean_absolute_error: 3.0315 - val_loss: 3.3126 - val_mean_absolute_error: 3.3126\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 3.28515\n",
            "Epoch 209/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0207 - mean_absolute_error: 3.0207 - val_loss: 3.3547 - val_mean_absolute_error: 3.3547\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 3.28515\n",
            "Epoch 210/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0319 - mean_absolute_error: 3.0319 - val_loss: 3.3239 - val_mean_absolute_error: 3.3239\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 3.28515\n",
            "Epoch 211/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 3.0428 - mean_absolute_error: 3.0428 - val_loss: 3.3147 - val_mean_absolute_error: 3.3147\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 3.28515\n",
            "Epoch 212/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0704 - mean_absolute_error: 3.0704 - val_loss: 3.3630 - val_mean_absolute_error: 3.3630\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 3.28515\n",
            "Epoch 213/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0356 - mean_absolute_error: 3.0356 - val_loss: 3.3614 - val_mean_absolute_error: 3.3614\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 3.28515\n",
            "Epoch 214/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0232 - mean_absolute_error: 3.0232 - val_loss: 3.3227 - val_mean_absolute_error: 3.3227\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 3.28515\n",
            "Epoch 215/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0105 - mean_absolute_error: 3.0105 - val_loss: 3.3135 - val_mean_absolute_error: 3.3135\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 3.28515\n",
            "Epoch 216/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0347 - mean_absolute_error: 3.0347 - val_loss: 3.2889 - val_mean_absolute_error: 3.2888\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 3.28515\n",
            "Epoch 217/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0338 - mean_absolute_error: 3.0338 - val_loss: 3.3692 - val_mean_absolute_error: 3.3692\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 3.28515\n",
            "Epoch 218/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 3.0681 - mean_absolute_error: 3.0681 - val_loss: 3.4144 - val_mean_absolute_error: 3.4144\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 3.28515\n",
            "Epoch 219/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0619 - mean_absolute_error: 3.0619 - val_loss: 3.3064 - val_mean_absolute_error: 3.3064\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 3.28515\n",
            "Epoch 220/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0534 - mean_absolute_error: 3.0534 - val_loss: 3.4395 - val_mean_absolute_error: 3.4395\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 3.28515\n",
            "Epoch 221/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0445 - mean_absolute_error: 3.0445 - val_loss: 3.2948 - val_mean_absolute_error: 3.2948\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 3.28515\n",
            "Epoch 222/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 3.0220 - mean_absolute_error: 3.0220 - val_loss: 3.4983 - val_mean_absolute_error: 3.4983\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 3.28515\n",
            "Epoch 223/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0734 - mean_absolute_error: 3.0734 - val_loss: 3.3607 - val_mean_absolute_error: 3.3607\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 3.28515\n",
            "Epoch 224/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0226 - mean_absolute_error: 3.0226 - val_loss: 3.3685 - val_mean_absolute_error: 3.3685\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 3.28515\n",
            "Epoch 225/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0182 - mean_absolute_error: 3.0182 - val_loss: 3.3197 - val_mean_absolute_error: 3.3197\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 3.28515\n",
            "Epoch 226/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9943 - mean_absolute_error: 2.9943 - val_loss: 3.2985 - val_mean_absolute_error: 3.2985\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 3.28515\n",
            "Epoch 227/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0084 - mean_absolute_error: 3.0084 - val_loss: 3.2877 - val_mean_absolute_error: 3.2877\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 3.28515\n",
            "Epoch 228/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0118 - mean_absolute_error: 3.0118 - val_loss: 3.4115 - val_mean_absolute_error: 3.4115\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 3.28515\n",
            "Epoch 229/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0535 - mean_absolute_error: 3.0535 - val_loss: 3.3626 - val_mean_absolute_error: 3.3626\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 3.28515\n",
            "Epoch 230/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0349 - mean_absolute_error: 3.0349 - val_loss: 3.3810 - val_mean_absolute_error: 3.3810\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 3.28515\n",
            "Epoch 231/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0525 - mean_absolute_error: 3.0525 - val_loss: 3.2642 - val_mean_absolute_error: 3.2642\n",
            "\n",
            "Epoch 00231: val_loss improved from 3.28515 to 3.26420, saving model to Weights-231--3.26420.hdf5\n",
            "Epoch 232/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0174 - mean_absolute_error: 3.0174 - val_loss: 3.3155 - val_mean_absolute_error: 3.3155\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 3.26420\n",
            "Epoch 233/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0322 - mean_absolute_error: 3.0322 - val_loss: 3.3337 - val_mean_absolute_error: 3.3337\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 3.26420\n",
            "Epoch 234/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0140 - mean_absolute_error: 3.0140 - val_loss: 3.3200 - val_mean_absolute_error: 3.3200\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 3.26420\n",
            "Epoch 235/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0008 - mean_absolute_error: 3.0008 - val_loss: 3.3310 - val_mean_absolute_error: 3.3310\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 3.26420\n",
            "Epoch 236/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0057 - mean_absolute_error: 3.0057 - val_loss: 3.2881 - val_mean_absolute_error: 3.2881\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 3.26420\n",
            "Epoch 237/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0081 - mean_absolute_error: 3.0081 - val_loss: 3.2787 - val_mean_absolute_error: 3.2787\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 3.26420\n",
            "Epoch 238/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0063 - mean_absolute_error: 3.0063 - val_loss: 3.2976 - val_mean_absolute_error: 3.2976\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 3.26420\n",
            "Epoch 239/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9845 - mean_absolute_error: 2.9845 - val_loss: 3.2576 - val_mean_absolute_error: 3.2576\n",
            "\n",
            "Epoch 00239: val_loss improved from 3.26420 to 3.25756, saving model to Weights-239--3.25756.hdf5\n",
            "Epoch 240/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9975 - mean_absolute_error: 2.9975 - val_loss: 3.3180 - val_mean_absolute_error: 3.3180\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 3.25756\n",
            "Epoch 241/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 2.9894 - mean_absolute_error: 2.9894 - val_loss: 3.2640 - val_mean_absolute_error: 3.2640\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 3.25756\n",
            "Epoch 242/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1147 - mean_absolute_error: 3.1147 - val_loss: 3.4686 - val_mean_absolute_error: 3.4686\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 3.25756\n",
            "Epoch 243/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0893 - mean_absolute_error: 3.0893 - val_loss: 3.3356 - val_mean_absolute_error: 3.3356\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 3.25756\n",
            "Epoch 244/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0025 - mean_absolute_error: 3.0025 - val_loss: 3.3014 - val_mean_absolute_error: 3.3014\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 3.25756\n",
            "Epoch 245/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0201 - mean_absolute_error: 3.0201 - val_loss: 3.2894 - val_mean_absolute_error: 3.2894\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 3.25756\n",
            "Epoch 246/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0070 - mean_absolute_error: 3.0070 - val_loss: 3.2879 - val_mean_absolute_error: 3.2879\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 3.25756\n",
            "Epoch 247/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0443 - mean_absolute_error: 3.0443 - val_loss: 3.2770 - val_mean_absolute_error: 3.2770\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 3.25756\n",
            "Epoch 248/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0493 - mean_absolute_error: 3.0493 - val_loss: 3.3914 - val_mean_absolute_error: 3.3914\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 3.25756\n",
            "Epoch 249/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0256 - mean_absolute_error: 3.0256 - val_loss: 3.2761 - val_mean_absolute_error: 3.2761\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 3.25756\n",
            "Epoch 250/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0291 - mean_absolute_error: 3.0291 - val_loss: 3.3128 - val_mean_absolute_error: 3.3128\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 3.25756\n",
            "Epoch 251/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0031 - mean_absolute_error: 3.0031 - val_loss: 3.3105 - val_mean_absolute_error: 3.3105\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 3.25756\n",
            "Epoch 252/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0335 - mean_absolute_error: 3.0335 - val_loss: 3.3388 - val_mean_absolute_error: 3.3388\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 3.25756\n",
            "Epoch 253/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9967 - mean_absolute_error: 2.9967 - val_loss: 3.3154 - val_mean_absolute_error: 3.3154\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 3.25756\n",
            "Epoch 254/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9971 - mean_absolute_error: 2.9971 - val_loss: 3.2670 - val_mean_absolute_error: 3.2670\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 3.25756\n",
            "Epoch 255/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0440 - mean_absolute_error: 3.0440 - val_loss: 3.4174 - val_mean_absolute_error: 3.4174\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 3.25756\n",
            "Epoch 256/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0094 - mean_absolute_error: 3.0094 - val_loss: 3.3075 - val_mean_absolute_error: 3.3075\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 3.25756\n",
            "Epoch 257/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0391 - mean_absolute_error: 3.0391 - val_loss: 3.3880 - val_mean_absolute_error: 3.3880\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 3.25756\n",
            "Epoch 258/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0297 - mean_absolute_error: 3.0297 - val_loss: 3.3159 - val_mean_absolute_error: 3.3159\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 3.25756\n",
            "Epoch 259/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0133 - mean_absolute_error: 3.0133 - val_loss: 3.2805 - val_mean_absolute_error: 3.2805\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 3.25756\n",
            "Epoch 260/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0102 - mean_absolute_error: 3.0102 - val_loss: 3.3838 - val_mean_absolute_error: 3.3838\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 3.25756\n",
            "Epoch 261/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0236 - mean_absolute_error: 3.0236 - val_loss: 3.2892 - val_mean_absolute_error: 3.2892\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 3.25756\n",
            "Epoch 262/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0324 - mean_absolute_error: 3.0324 - val_loss: 3.3894 - val_mean_absolute_error: 3.3894\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 3.25756\n",
            "Epoch 263/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0282 - mean_absolute_error: 3.0282 - val_loss: 3.3227 - val_mean_absolute_error: 3.3227\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 3.25756\n",
            "Epoch 264/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0071 - mean_absolute_error: 3.0071 - val_loss: 3.3151 - val_mean_absolute_error: 3.3151\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 3.25756\n",
            "Epoch 265/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0079 - mean_absolute_error: 3.0079 - val_loss: 3.3315 - val_mean_absolute_error: 3.3315\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 3.25756\n",
            "Epoch 266/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 3.0078 - mean_absolute_error: 3.0078 - val_loss: 3.2728 - val_mean_absolute_error: 3.2728\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 3.25756\n",
            "Epoch 267/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0123 - mean_absolute_error: 3.0123 - val_loss: 3.3113 - val_mean_absolute_error: 3.3113\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 3.25756\n",
            "Epoch 268/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0469 - mean_absolute_error: 3.0469 - val_loss: 3.3698 - val_mean_absolute_error: 3.3698\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 3.25756\n",
            "Epoch 269/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0006 - mean_absolute_error: 3.0006 - val_loss: 3.2512 - val_mean_absolute_error: 3.2512\n",
            "\n",
            "Epoch 00269: val_loss improved from 3.25756 to 3.25117, saving model to Weights-269--3.25117.hdf5\n",
            "Epoch 270/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9952 - mean_absolute_error: 2.9952 - val_loss: 3.3003 - val_mean_absolute_error: 3.3003\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 3.25117\n",
            "Epoch 271/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0047 - mean_absolute_error: 3.0047 - val_loss: 3.2860 - val_mean_absolute_error: 3.2860\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 3.25117\n",
            "Epoch 272/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0275 - mean_absolute_error: 3.0275 - val_loss: 3.3640 - val_mean_absolute_error: 3.3640\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 3.25117\n",
            "Epoch 273/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9843 - mean_absolute_error: 2.9843 - val_loss: 3.2772 - val_mean_absolute_error: 3.2772\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 3.25117\n",
            "Epoch 274/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9906 - mean_absolute_error: 2.9906 - val_loss: 3.3096 - val_mean_absolute_error: 3.3096\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 3.25117\n",
            "Epoch 275/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9830 - mean_absolute_error: 2.9830 - val_loss: 3.2697 - val_mean_absolute_error: 3.2697\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 3.25117\n",
            "Epoch 276/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 2.9981 - mean_absolute_error: 2.9981 - val_loss: 3.2876 - val_mean_absolute_error: 3.2876\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 3.25117\n",
            "Epoch 277/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9932 - mean_absolute_error: 2.9932 - val_loss: 3.2753 - val_mean_absolute_error: 3.2753\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 3.25117\n",
            "Epoch 278/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9928 - mean_absolute_error: 2.9928 - val_loss: 3.3233 - val_mean_absolute_error: 3.3233\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 3.25117\n",
            "Epoch 279/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0240 - mean_absolute_error: 3.0240 - val_loss: 3.4650 - val_mean_absolute_error: 3.4650\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 3.25117\n",
            "Epoch 280/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0684 - mean_absolute_error: 3.0684 - val_loss: 3.3368 - val_mean_absolute_error: 3.3368\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 3.25117\n",
            "Epoch 281/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0136 - mean_absolute_error: 3.0136 - val_loss: 3.3411 - val_mean_absolute_error: 3.3411\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 3.25117\n",
            "Epoch 282/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0244 - mean_absolute_error: 3.0244 - val_loss: 3.2332 - val_mean_absolute_error: 3.2332\n",
            "\n",
            "Epoch 00282: val_loss improved from 3.25117 to 3.23321, saving model to Weights-282--3.23321.hdf5\n",
            "Epoch 283/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0294 - mean_absolute_error: 3.0294 - val_loss: 3.3358 - val_mean_absolute_error: 3.3358\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 3.23321\n",
            "Epoch 284/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9980 - mean_absolute_error: 2.9980 - val_loss: 3.2796 - val_mean_absolute_error: 3.2796\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 3.23321\n",
            "Epoch 285/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0423 - mean_absolute_error: 3.0423 - val_loss: 3.3968 - val_mean_absolute_error: 3.3968\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 3.23321\n",
            "Epoch 286/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0310 - mean_absolute_error: 3.0310 - val_loss: 3.2747 - val_mean_absolute_error: 3.2747\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 3.23321\n",
            "Epoch 287/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0413 - mean_absolute_error: 3.0413 - val_loss: 3.3802 - val_mean_absolute_error: 3.3802\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 3.23321\n",
            "Epoch 288/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0711 - mean_absolute_error: 3.0711 - val_loss: 3.4156 - val_mean_absolute_error: 3.4156\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 3.23321\n",
            "Epoch 289/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0387 - mean_absolute_error: 3.0387 - val_loss: 3.2725 - val_mean_absolute_error: 3.2725\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 3.23321\n",
            "Epoch 290/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9878 - mean_absolute_error: 2.9878 - val_loss: 3.3397 - val_mean_absolute_error: 3.3397\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 3.23321\n",
            "Epoch 291/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0066 - mean_absolute_error: 3.0066 - val_loss: 3.2830 - val_mean_absolute_error: 3.2830\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 3.23321\n",
            "Epoch 292/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0041 - mean_absolute_error: 3.0041 - val_loss: 3.2938 - val_mean_absolute_error: 3.2938\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 3.23321\n",
            "Epoch 293/500\n",
            "1376/1376 [==============================] - 0s 86us/step - loss: 3.0179 - mean_absolute_error: 3.0179 - val_loss: 3.3132 - val_mean_absolute_error: 3.3132\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 3.23321\n",
            "Epoch 294/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0035 - mean_absolute_error: 3.0035 - val_loss: 3.2752 - val_mean_absolute_error: 3.2752\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 3.23321\n",
            "Epoch 295/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9853 - mean_absolute_error: 2.9853 - val_loss: 3.2816 - val_mean_absolute_error: 3.2816\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 3.23321\n",
            "Epoch 296/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9789 - mean_absolute_error: 2.9789 - val_loss: 3.2536 - val_mean_absolute_error: 3.2536\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 3.23321\n",
            "Epoch 297/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9905 - mean_absolute_error: 2.9905 - val_loss: 3.2689 - val_mean_absolute_error: 3.2689\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 3.23321\n",
            "Epoch 298/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9839 - mean_absolute_error: 2.9839 - val_loss: 3.2602 - val_mean_absolute_error: 3.2602\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 3.23321\n",
            "Epoch 299/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0327 - mean_absolute_error: 3.0327 - val_loss: 3.3603 - val_mean_absolute_error: 3.3603\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 3.23321\n",
            "Epoch 300/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0061 - mean_absolute_error: 3.0061 - val_loss: 3.2652 - val_mean_absolute_error: 3.2652\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 3.23321\n",
            "Epoch 301/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.0134 - mean_absolute_error: 3.0134 - val_loss: 3.3021 - val_mean_absolute_error: 3.3021\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 3.23321\n",
            "Epoch 302/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0267 - mean_absolute_error: 3.0267 - val_loss: 3.2784 - val_mean_absolute_error: 3.2784\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 3.23321\n",
            "Epoch 303/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0554 - mean_absolute_error: 3.0554 - val_loss: 3.3156 - val_mean_absolute_error: 3.3156\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 3.23321\n",
            "Epoch 304/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9904 - mean_absolute_error: 2.9904 - val_loss: 3.3208 - val_mean_absolute_error: 3.3208\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 3.23321\n",
            "Epoch 305/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9839 - mean_absolute_error: 2.9839 - val_loss: 3.2669 - val_mean_absolute_error: 3.2669\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 3.23321\n",
            "Epoch 306/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0057 - mean_absolute_error: 3.0057 - val_loss: 3.3329 - val_mean_absolute_error: 3.3329\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 3.23321\n",
            "Epoch 307/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9862 - mean_absolute_error: 2.9862 - val_loss: 3.2385 - val_mean_absolute_error: 3.2385\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 3.23321\n",
            "Epoch 308/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 2.9749 - mean_absolute_error: 2.9749 - val_loss: 3.2329 - val_mean_absolute_error: 3.2329\n",
            "\n",
            "Epoch 00308: val_loss improved from 3.23321 to 3.23288, saving model to Weights-308--3.23288.hdf5\n",
            "Epoch 309/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0437 - mean_absolute_error: 3.0437 - val_loss: 3.4090 - val_mean_absolute_error: 3.4090\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 3.23288\n",
            "Epoch 310/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9924 - mean_absolute_error: 2.9924 - val_loss: 3.2232 - val_mean_absolute_error: 3.2232\n",
            "\n",
            "Epoch 00310: val_loss improved from 3.23288 to 3.22315, saving model to Weights-310--3.22315.hdf5\n",
            "Epoch 311/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 2.9581 - mean_absolute_error: 2.9581 - val_loss: 3.3566 - val_mean_absolute_error: 3.3566\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 3.22315\n",
            "Epoch 312/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0151 - mean_absolute_error: 3.0151 - val_loss: 3.3002 - val_mean_absolute_error: 3.3002\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 3.22315\n",
            "Epoch 313/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9871 - mean_absolute_error: 2.9871 - val_loss: 3.2636 - val_mean_absolute_error: 3.2636\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 3.22315\n",
            "Epoch 314/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9918 - mean_absolute_error: 2.9918 - val_loss: 3.2746 - val_mean_absolute_error: 3.2746\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 3.22315\n",
            "Epoch 315/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9753 - mean_absolute_error: 2.9753 - val_loss: 3.2829 - val_mean_absolute_error: 3.2829\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 3.22315\n",
            "Epoch 316/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9715 - mean_absolute_error: 2.9715 - val_loss: 3.3677 - val_mean_absolute_error: 3.3677\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 3.22315\n",
            "Epoch 317/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0709 - mean_absolute_error: 3.0709 - val_loss: 3.4140 - val_mean_absolute_error: 3.4140\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 3.22315\n",
            "Epoch 318/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0491 - mean_absolute_error: 3.0491 - val_loss: 3.3021 - val_mean_absolute_error: 3.3021\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 3.22315\n",
            "Epoch 319/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0132 - mean_absolute_error: 3.0132 - val_loss: 3.2693 - val_mean_absolute_error: 3.2693\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 3.22315\n",
            "Epoch 320/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9763 - mean_absolute_error: 2.9763 - val_loss: 3.3582 - val_mean_absolute_error: 3.3582\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 3.22315\n",
            "Epoch 321/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0163 - mean_absolute_error: 3.0163 - val_loss: 3.2862 - val_mean_absolute_error: 3.2862\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 3.22315\n",
            "Epoch 322/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9821 - mean_absolute_error: 2.9821 - val_loss: 3.2538 - val_mean_absolute_error: 3.2538\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 3.22315\n",
            "Epoch 323/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0104 - mean_absolute_error: 3.0104 - val_loss: 3.4622 - val_mean_absolute_error: 3.4622\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 3.22315\n",
            "Epoch 324/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9914 - mean_absolute_error: 2.9914 - val_loss: 3.2954 - val_mean_absolute_error: 3.2954\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 3.22315\n",
            "Epoch 325/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 2.9711 - mean_absolute_error: 2.9711 - val_loss: 3.3116 - val_mean_absolute_error: 3.3116\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 3.22315\n",
            "Epoch 326/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 3.0297 - mean_absolute_error: 3.0297 - val_loss: 3.2549 - val_mean_absolute_error: 3.2549\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 3.22315\n",
            "Epoch 327/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9703 - mean_absolute_error: 2.9703 - val_loss: 3.2379 - val_mean_absolute_error: 3.2379\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 3.22315\n",
            "Epoch 328/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0114 - mean_absolute_error: 3.0114 - val_loss: 3.3433 - val_mean_absolute_error: 3.3433\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 3.22315\n",
            "Epoch 329/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0140 - mean_absolute_error: 3.0140 - val_loss: 3.2335 - val_mean_absolute_error: 3.2335\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 3.22315\n",
            "Epoch 330/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9923 - mean_absolute_error: 2.9923 - val_loss: 3.2622 - val_mean_absolute_error: 3.2622\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 3.22315\n",
            "Epoch 331/500\n",
            "1376/1376 [==============================] - 0s 87us/step - loss: 3.0071 - mean_absolute_error: 3.0071 - val_loss: 3.2494 - val_mean_absolute_error: 3.2494\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 3.22315\n",
            "Epoch 332/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9689 - mean_absolute_error: 2.9689 - val_loss: 3.2572 - val_mean_absolute_error: 3.2572\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 3.22315\n",
            "Epoch 333/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 2.9867 - mean_absolute_error: 2.9867 - val_loss: 3.2582 - val_mean_absolute_error: 3.2582\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 3.22315\n",
            "Epoch 334/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9798 - mean_absolute_error: 2.9798 - val_loss: 3.2428 - val_mean_absolute_error: 3.2428\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 3.22315\n",
            "Epoch 335/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0000 - mean_absolute_error: 3.0000 - val_loss: 3.2054 - val_mean_absolute_error: 3.2054\n",
            "\n",
            "Epoch 00335: val_loss improved from 3.22315 to 3.20544, saving model to Weights-335--3.20544.hdf5\n",
            "Epoch 336/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9743 - mean_absolute_error: 2.9743 - val_loss: 3.2784 - val_mean_absolute_error: 3.2784\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 3.20544\n",
            "Epoch 337/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9803 - mean_absolute_error: 2.9803 - val_loss: 3.4300 - val_mean_absolute_error: 3.4300\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 3.20544\n",
            "Epoch 338/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0462 - mean_absolute_error: 3.0462 - val_loss: 3.2722 - val_mean_absolute_error: 3.2722\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 3.20544\n",
            "Epoch 339/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 2.9847 - mean_absolute_error: 2.9847 - val_loss: 3.2505 - val_mean_absolute_error: 3.2505\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 3.20544\n",
            "Epoch 340/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0066 - mean_absolute_error: 3.0066 - val_loss: 3.2822 - val_mean_absolute_error: 3.2822\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 3.20544\n",
            "Epoch 341/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9969 - mean_absolute_error: 2.9969 - val_loss: 3.2473 - val_mean_absolute_error: 3.2473\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 3.20544\n",
            "Epoch 342/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9842 - mean_absolute_error: 2.9842 - val_loss: 3.2131 - val_mean_absolute_error: 3.2131\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 3.20544\n",
            "Epoch 343/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 2.9916 - mean_absolute_error: 2.9916 - val_loss: 3.2012 - val_mean_absolute_error: 3.2012\n",
            "\n",
            "Epoch 00343: val_loss improved from 3.20544 to 3.20123, saving model to Weights-343--3.20123.hdf5\n",
            "Epoch 344/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 3.0054 - mean_absolute_error: 3.0054 - val_loss: 3.2863 - val_mean_absolute_error: 3.2863\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 3.20123\n",
            "Epoch 345/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9926 - mean_absolute_error: 2.9926 - val_loss: 3.2768 - val_mean_absolute_error: 3.2768\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 3.20123\n",
            "Epoch 346/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9798 - mean_absolute_error: 2.9798 - val_loss: 3.2931 - val_mean_absolute_error: 3.2931\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 3.20123\n",
            "Epoch 347/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9600 - mean_absolute_error: 2.9600 - val_loss: 3.2279 - val_mean_absolute_error: 3.2279\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 3.20123\n",
            "Epoch 348/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9797 - mean_absolute_error: 2.9797 - val_loss: 3.2699 - val_mean_absolute_error: 3.2699\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 3.20123\n",
            "Epoch 349/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9771 - mean_absolute_error: 2.9771 - val_loss: 3.2620 - val_mean_absolute_error: 3.2620\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 3.20123\n",
            "Epoch 350/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9865 - mean_absolute_error: 2.9865 - val_loss: 3.2904 - val_mean_absolute_error: 3.2904\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 3.20123\n",
            "Epoch 351/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9820 - mean_absolute_error: 2.9820 - val_loss: 3.2404 - val_mean_absolute_error: 3.2404\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 3.20123\n",
            "Epoch 352/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9659 - mean_absolute_error: 2.9659 - val_loss: 3.2140 - val_mean_absolute_error: 3.2140\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 3.20123\n",
            "Epoch 353/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9718 - mean_absolute_error: 2.9718 - val_loss: 3.2143 - val_mean_absolute_error: 3.2143\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 3.20123\n",
            "Epoch 354/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9554 - mean_absolute_error: 2.9554 - val_loss: 3.2304 - val_mean_absolute_error: 3.2304\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 3.20123\n",
            "Epoch 355/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9694 - mean_absolute_error: 2.9694 - val_loss: 3.2234 - val_mean_absolute_error: 3.2234\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 3.20123\n",
            "Epoch 356/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0063 - mean_absolute_error: 3.0063 - val_loss: 3.2867 - val_mean_absolute_error: 3.2867\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 3.20123\n",
            "Epoch 357/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9659 - mean_absolute_error: 2.9659 - val_loss: 3.2317 - val_mean_absolute_error: 3.2317\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 3.20123\n",
            "Epoch 358/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9689 - mean_absolute_error: 2.9689 - val_loss: 3.2126 - val_mean_absolute_error: 3.2126\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 3.20123\n",
            "Epoch 359/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9751 - mean_absolute_error: 2.9751 - val_loss: 3.2471 - val_mean_absolute_error: 3.2471\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 3.20123\n",
            "Epoch 360/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 2.9421 - mean_absolute_error: 2.9421 - val_loss: 3.5074 - val_mean_absolute_error: 3.5074\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 3.20123\n",
            "Epoch 361/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.2097 - mean_absolute_error: 3.2097 - val_loss: 3.5652 - val_mean_absolute_error: 3.5652\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 3.20123\n",
            "Epoch 362/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.1371 - mean_absolute_error: 3.1371 - val_loss: 3.5041 - val_mean_absolute_error: 3.5041\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 3.20123\n",
            "Epoch 363/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 3.0970 - mean_absolute_error: 3.0970 - val_loss: 3.4660 - val_mean_absolute_error: 3.4660\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 3.20123\n",
            "Epoch 364/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0705 - mean_absolute_error: 3.0705 - val_loss: 3.4239 - val_mean_absolute_error: 3.4239\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 3.20123\n",
            "Epoch 365/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0674 - mean_absolute_error: 3.0674 - val_loss: 3.4107 - val_mean_absolute_error: 3.4107\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 3.20123\n",
            "Epoch 366/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0470 - mean_absolute_error: 3.0470 - val_loss: 3.3739 - val_mean_absolute_error: 3.3739\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 3.20123\n",
            "Epoch 367/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0354 - mean_absolute_error: 3.0354 - val_loss: 3.3594 - val_mean_absolute_error: 3.3594\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 3.20123\n",
            "Epoch 368/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0110 - mean_absolute_error: 3.0110 - val_loss: 3.3286 - val_mean_absolute_error: 3.3286\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 3.20123\n",
            "Epoch 369/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0091 - mean_absolute_error: 3.0091 - val_loss: 3.3573 - val_mean_absolute_error: 3.3573\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 3.20123\n",
            "Epoch 370/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0028 - mean_absolute_error: 3.0028 - val_loss: 3.2907 - val_mean_absolute_error: 3.2907\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 3.20123\n",
            "Epoch 371/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9833 - mean_absolute_error: 2.9833 - val_loss: 3.3192 - val_mean_absolute_error: 3.3192\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 3.20123\n",
            "Epoch 372/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9858 - mean_absolute_error: 2.9858 - val_loss: 3.2916 - val_mean_absolute_error: 3.2916\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 3.20123\n",
            "Epoch 373/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9972 - mean_absolute_error: 2.9972 - val_loss: 3.3536 - val_mean_absolute_error: 3.3536\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 3.20123\n",
            "Epoch 374/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 3.0008 - mean_absolute_error: 3.0008 - val_loss: 3.2983 - val_mean_absolute_error: 3.2983\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 3.20123\n",
            "Epoch 375/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0474 - mean_absolute_error: 3.0474 - val_loss: 3.2893 - val_mean_absolute_error: 3.2893\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 3.20123\n",
            "Epoch 376/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0179 - mean_absolute_error: 3.0179 - val_loss: 3.3923 - val_mean_absolute_error: 3.3923\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 3.20123\n",
            "Epoch 377/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9908 - mean_absolute_error: 2.9908 - val_loss: 3.2901 - val_mean_absolute_error: 3.2901\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 3.20123\n",
            "Epoch 378/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9941 - mean_absolute_error: 2.9941 - val_loss: 3.2755 - val_mean_absolute_error: 3.2755\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 3.20123\n",
            "Epoch 379/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0078 - mean_absolute_error: 3.0078 - val_loss: 3.2792 - val_mean_absolute_error: 3.2792\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 3.20123\n",
            "Epoch 380/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9766 - mean_absolute_error: 2.9766 - val_loss: 3.3620 - val_mean_absolute_error: 3.3620\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 3.20123\n",
            "Epoch 381/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9799 - mean_absolute_error: 2.9799 - val_loss: 3.2905 - val_mean_absolute_error: 3.2905\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 3.20123\n",
            "Epoch 382/500\n",
            "1376/1376 [==============================] - 0s 82us/step - loss: 3.0175 - mean_absolute_error: 3.0175 - val_loss: 3.4479 - val_mean_absolute_error: 3.4479\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 3.20123\n",
            "Epoch 383/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0436 - mean_absolute_error: 3.0436 - val_loss: 3.3281 - val_mean_absolute_error: 3.3281\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 3.20123\n",
            "Epoch 384/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9851 - mean_absolute_error: 2.9851 - val_loss: 3.3270 - val_mean_absolute_error: 3.3270\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 3.20123\n",
            "Epoch 385/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9785 - mean_absolute_error: 2.9785 - val_loss: 3.2714 - val_mean_absolute_error: 3.2714\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 3.20123\n",
            "Epoch 386/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 2.9875 - mean_absolute_error: 2.9875 - val_loss: 3.2668 - val_mean_absolute_error: 3.2668\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 3.20123\n",
            "Epoch 387/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9832 - mean_absolute_error: 2.9832 - val_loss: 3.2570 - val_mean_absolute_error: 3.2570\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 3.20123\n",
            "Epoch 388/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9900 - mean_absolute_error: 2.9900 - val_loss: 3.2726 - val_mean_absolute_error: 3.2726\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 3.20123\n",
            "Epoch 389/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9935 - mean_absolute_error: 2.9935 - val_loss: 3.2332 - val_mean_absolute_error: 3.2332\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 3.20123\n",
            "Epoch 390/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0260 - mean_absolute_error: 3.0260 - val_loss: 3.3418 - val_mean_absolute_error: 3.3418\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 3.20123\n",
            "Epoch 391/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9669 - mean_absolute_error: 2.9669 - val_loss: 3.2873 - val_mean_absolute_error: 3.2873\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 3.20123\n",
            "Epoch 392/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9742 - mean_absolute_error: 2.9742 - val_loss: 3.2850 - val_mean_absolute_error: 3.2850\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 3.20123\n",
            "Epoch 393/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9601 - mean_absolute_error: 2.9601 - val_loss: 3.2538 - val_mean_absolute_error: 3.2538\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 3.20123\n",
            "Epoch 394/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9746 - mean_absolute_error: 2.9746 - val_loss: 3.2795 - val_mean_absolute_error: 3.2795\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 3.20123\n",
            "Epoch 395/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9854 - mean_absolute_error: 2.9854 - val_loss: 3.2303 - val_mean_absolute_error: 3.2303\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 3.20123\n",
            "Epoch 396/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9666 - mean_absolute_error: 2.9666 - val_loss: 3.2610 - val_mean_absolute_error: 3.2610\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 3.20123\n",
            "Epoch 397/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9899 - mean_absolute_error: 2.9899 - val_loss: 3.3421 - val_mean_absolute_error: 3.3421\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 3.20123\n",
            "Epoch 398/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0237 - mean_absolute_error: 3.0237 - val_loss: 3.4092 - val_mean_absolute_error: 3.4092\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 3.20123\n",
            "Epoch 399/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.0095 - mean_absolute_error: 3.0095 - val_loss: 3.3066 - val_mean_absolute_error: 3.3066\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 3.20123\n",
            "Epoch 400/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9549 - mean_absolute_error: 2.9549 - val_loss: 3.2008 - val_mean_absolute_error: 3.2008\n",
            "\n",
            "Epoch 00400: val_loss improved from 3.20123 to 3.20083, saving model to Weights-400--3.20083.hdf5\n",
            "Epoch 401/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9603 - mean_absolute_error: 2.9603 - val_loss: 3.2514 - val_mean_absolute_error: 3.2514\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 3.20083\n",
            "Epoch 402/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9632 - mean_absolute_error: 2.9632 - val_loss: 3.2564 - val_mean_absolute_error: 3.2564\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 3.20083\n",
            "Epoch 403/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 2.9749 - mean_absolute_error: 2.9749 - val_loss: 3.3336 - val_mean_absolute_error: 3.3336\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 3.20083\n",
            "Epoch 404/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9998 - mean_absolute_error: 2.9998 - val_loss: 3.3985 - val_mean_absolute_error: 3.3985\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 3.20083\n",
            "Epoch 405/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0208 - mean_absolute_error: 3.0208 - val_loss: 3.2202 - val_mean_absolute_error: 3.2202\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 3.20083\n",
            "Epoch 406/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9857 - mean_absolute_error: 2.9857 - val_loss: 3.3116 - val_mean_absolute_error: 3.3116\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 3.20083\n",
            "Epoch 407/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 2.9854 - mean_absolute_error: 2.9854 - val_loss: 3.3243 - val_mean_absolute_error: 3.3243\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 3.20083\n",
            "Epoch 408/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0074 - mean_absolute_error: 3.0074 - val_loss: 3.2619 - val_mean_absolute_error: 3.2619\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 3.20083\n",
            "Epoch 409/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 2.9795 - mean_absolute_error: 2.9795 - val_loss: 3.2532 - val_mean_absolute_error: 3.2532\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 3.20083\n",
            "Epoch 410/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9642 - mean_absolute_error: 2.9642 - val_loss: 3.2531 - val_mean_absolute_error: 3.2531\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 3.20083\n",
            "Epoch 411/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9599 - mean_absolute_error: 2.9599 - val_loss: 3.2458 - val_mean_absolute_error: 3.2458\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 3.20083\n",
            "Epoch 412/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 3.0136 - mean_absolute_error: 3.0136 - val_loss: 3.2606 - val_mean_absolute_error: 3.2606\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 3.20083\n",
            "Epoch 413/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9507 - mean_absolute_error: 2.9507 - val_loss: 3.2973 - val_mean_absolute_error: 3.2973\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 3.20083\n",
            "Epoch 414/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0172 - mean_absolute_error: 3.0172 - val_loss: 3.3231 - val_mean_absolute_error: 3.3231\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 3.20083\n",
            "Epoch 415/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9707 - mean_absolute_error: 2.9707 - val_loss: 3.2479 - val_mean_absolute_error: 3.2479\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 3.20083\n",
            "Epoch 416/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 2.9573 - mean_absolute_error: 2.9573 - val_loss: 3.2317 - val_mean_absolute_error: 3.2317\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 3.20083\n",
            "Epoch 417/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9537 - mean_absolute_error: 2.9537 - val_loss: 3.3774 - val_mean_absolute_error: 3.3774\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 3.20083\n",
            "Epoch 418/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9642 - mean_absolute_error: 2.9642 - val_loss: 3.2000 - val_mean_absolute_error: 3.2000\n",
            "\n",
            "Epoch 00418: val_loss improved from 3.20083 to 3.19997, saving model to Weights-418--3.19997.hdf5\n",
            "Epoch 419/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9762 - mean_absolute_error: 2.9762 - val_loss: 3.2291 - val_mean_absolute_error: 3.2291\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 3.19997\n",
            "Epoch 420/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9644 - mean_absolute_error: 2.9644 - val_loss: 3.3050 - val_mean_absolute_error: 3.3050\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 3.19997\n",
            "Epoch 421/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9700 - mean_absolute_error: 2.9700 - val_loss: 3.2288 - val_mean_absolute_error: 3.2288\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 3.19997\n",
            "Epoch 422/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0037 - mean_absolute_error: 3.0037 - val_loss: 3.4930 - val_mean_absolute_error: 3.4930\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 3.19997\n",
            "Epoch 423/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0675 - mean_absolute_error: 3.0675 - val_loss: 3.2998 - val_mean_absolute_error: 3.2998\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 3.19997\n",
            "Epoch 424/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9778 - mean_absolute_error: 2.9778 - val_loss: 3.2945 - val_mean_absolute_error: 3.2945\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 3.19997\n",
            "Epoch 425/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9682 - mean_absolute_error: 2.9682 - val_loss: 3.2659 - val_mean_absolute_error: 3.2659\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 3.19997\n",
            "Epoch 426/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 2.9518 - mean_absolute_error: 2.9518 - val_loss: 3.2434 - val_mean_absolute_error: 3.2434\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 3.19997\n",
            "Epoch 427/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 2.9481 - mean_absolute_error: 2.9481 - val_loss: 3.1987 - val_mean_absolute_error: 3.1987\n",
            "\n",
            "Epoch 00427: val_loss improved from 3.19997 to 3.19869, saving model to Weights-427--3.19869.hdf5\n",
            "Epoch 428/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9700 - mean_absolute_error: 2.9700 - val_loss: 3.2508 - val_mean_absolute_error: 3.2508\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 3.19869\n",
            "Epoch 429/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9793 - mean_absolute_error: 2.9793 - val_loss: 3.2735 - val_mean_absolute_error: 3.2735\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 3.19869\n",
            "Epoch 430/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 2.9719 - mean_absolute_error: 2.9719 - val_loss: 3.2491 - val_mean_absolute_error: 3.2491\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 3.19869\n",
            "Epoch 431/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9720 - mean_absolute_error: 2.9720 - val_loss: 3.3082 - val_mean_absolute_error: 3.3082\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 3.19869\n",
            "Epoch 432/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9424 - mean_absolute_error: 2.9424 - val_loss: 3.2236 - val_mean_absolute_error: 3.2236\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 3.19869\n",
            "Epoch 433/500\n",
            "1376/1376 [==============================] - 0s 103us/step - loss: 2.9804 - mean_absolute_error: 2.9804 - val_loss: 3.2360 - val_mean_absolute_error: 3.2360\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 3.19869\n",
            "Epoch 434/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9714 - mean_absolute_error: 2.9714 - val_loss: 3.2488 - val_mean_absolute_error: 3.2488\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 3.19869\n",
            "Epoch 435/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9714 - mean_absolute_error: 2.9714 - val_loss: 3.1841 - val_mean_absolute_error: 3.1841\n",
            "\n",
            "Epoch 00435: val_loss improved from 3.19869 to 3.18406, saving model to Weights-435--3.18406.hdf5\n",
            "Epoch 436/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9424 - mean_absolute_error: 2.9424 - val_loss: 3.2907 - val_mean_absolute_error: 3.2907\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 3.18406\n",
            "Epoch 437/500\n",
            "1376/1376 [==============================] - 0s 68us/step - loss: 2.9811 - mean_absolute_error: 2.9811 - val_loss: 3.2506 - val_mean_absolute_error: 3.2506\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 3.18406\n",
            "Epoch 438/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9495 - mean_absolute_error: 2.9495 - val_loss: 3.2689 - val_mean_absolute_error: 3.2689\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 3.18406\n",
            "Epoch 439/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 2.9671 - mean_absolute_error: 2.9671 - val_loss: 3.2305 - val_mean_absolute_error: 3.2305\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 3.18406\n",
            "Epoch 440/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9659 - mean_absolute_error: 2.9659 - val_loss: 3.2261 - val_mean_absolute_error: 3.2261\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 3.18406\n",
            "Epoch 441/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9853 - mean_absolute_error: 2.9853 - val_loss: 3.3104 - val_mean_absolute_error: 3.3104\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 3.18406\n",
            "Epoch 442/500\n",
            "1376/1376 [==============================] - 0s 67us/step - loss: 2.9961 - mean_absolute_error: 2.9961 - val_loss: 3.1949 - val_mean_absolute_error: 3.1949\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 3.18406\n",
            "Epoch 443/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9469 - mean_absolute_error: 2.9469 - val_loss: 3.2692 - val_mean_absolute_error: 3.2692\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 3.18406\n",
            "Epoch 444/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9459 - mean_absolute_error: 2.9459 - val_loss: 3.2259 - val_mean_absolute_error: 3.2259\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 3.18406\n",
            "Epoch 445/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9895 - mean_absolute_error: 2.9895 - val_loss: 3.1977 - val_mean_absolute_error: 3.1977\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 3.18406\n",
            "Epoch 446/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9843 - mean_absolute_error: 2.9843 - val_loss: 3.3091 - val_mean_absolute_error: 3.3091\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 3.18406\n",
            "Epoch 447/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0009 - mean_absolute_error: 3.0009 - val_loss: 3.2239 - val_mean_absolute_error: 3.2239\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 3.18406\n",
            "Epoch 448/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9500 - mean_absolute_error: 2.9500 - val_loss: 3.2328 - val_mean_absolute_error: 3.2328\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 3.18406\n",
            "Epoch 449/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9698 - mean_absolute_error: 2.9698 - val_loss: 3.2557 - val_mean_absolute_error: 3.2557\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 3.18406\n",
            "Epoch 450/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0537 - mean_absolute_error: 3.0537 - val_loss: 3.2558 - val_mean_absolute_error: 3.2558\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 3.18406\n",
            "Epoch 451/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.1659 - mean_absolute_error: 3.1659 - val_loss: 3.5783 - val_mean_absolute_error: 3.5783\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 3.18406\n",
            "Epoch 452/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 3.1536 - mean_absolute_error: 3.1536 - val_loss: 3.5470 - val_mean_absolute_error: 3.5470\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 3.18406\n",
            "Epoch 453/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1209 - mean_absolute_error: 3.1209 - val_loss: 3.5035 - val_mean_absolute_error: 3.5035\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 3.18406\n",
            "Epoch 454/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0887 - mean_absolute_error: 3.0887 - val_loss: 3.4425 - val_mean_absolute_error: 3.4425\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 3.18406\n",
            "Epoch 455/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.0688 - mean_absolute_error: 3.0688 - val_loss: 3.3972 - val_mean_absolute_error: 3.3972\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 3.18406\n",
            "Epoch 456/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 3.0647 - mean_absolute_error: 3.0647 - val_loss: 3.3845 - val_mean_absolute_error: 3.3845\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 3.18406\n",
            "Epoch 457/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0564 - mean_absolute_error: 3.0564 - val_loss: 3.3757 - val_mean_absolute_error: 3.3757\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 3.18406\n",
            "Epoch 458/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 3.0365 - mean_absolute_error: 3.0365 - val_loss: 3.3634 - val_mean_absolute_error: 3.3634\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 3.18406\n",
            "Epoch 459/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0282 - mean_absolute_error: 3.0282 - val_loss: 3.3564 - val_mean_absolute_error: 3.3564\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 3.18406\n",
            "Epoch 460/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0258 - mean_absolute_error: 3.0258 - val_loss: 3.3640 - val_mean_absolute_error: 3.3640\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 3.18406\n",
            "Epoch 461/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 3.0208 - mean_absolute_error: 3.0208 - val_loss: 3.3261 - val_mean_absolute_error: 3.3261\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 3.18406\n",
            "Epoch 462/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.0216 - mean_absolute_error: 3.0216 - val_loss: 3.3280 - val_mean_absolute_error: 3.3280\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 3.18406\n",
            "Epoch 463/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0157 - mean_absolute_error: 3.0157 - val_loss: 3.3363 - val_mean_absolute_error: 3.3363\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 3.18406\n",
            "Epoch 464/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9996 - mean_absolute_error: 2.9996 - val_loss: 3.3022 - val_mean_absolute_error: 3.3022\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 3.18406\n",
            "Epoch 465/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 3.0222 - mean_absolute_error: 3.0222 - val_loss: 3.3811 - val_mean_absolute_error: 3.3811\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 3.18406\n",
            "Epoch 466/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 3.0003 - mean_absolute_error: 3.0003 - val_loss: 3.3046 - val_mean_absolute_error: 3.3046\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 3.18406\n",
            "Epoch 467/500\n",
            "1376/1376 [==============================] - 0s 85us/step - loss: 3.0085 - mean_absolute_error: 3.0085 - val_loss: 3.2621 - val_mean_absolute_error: 3.2621\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 3.18406\n",
            "Epoch 468/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9867 - mean_absolute_error: 2.9867 - val_loss: 3.2741 - val_mean_absolute_error: 3.2741\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 3.18406\n",
            "Epoch 469/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9899 - mean_absolute_error: 2.9899 - val_loss: 3.2983 - val_mean_absolute_error: 3.2983\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 3.18406\n",
            "Epoch 470/500\n",
            "1376/1376 [==============================] - 0s 69us/step - loss: 2.9714 - mean_absolute_error: 2.9714 - val_loss: 3.2632 - val_mean_absolute_error: 3.2632\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 3.18406\n",
            "Epoch 471/500\n",
            "1376/1376 [==============================] - 0s 78us/step - loss: 2.9797 - mean_absolute_error: 2.9797 - val_loss: 3.2625 - val_mean_absolute_error: 3.2625\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 3.18406\n",
            "Epoch 472/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9747 - mean_absolute_error: 2.9747 - val_loss: 3.2833 - val_mean_absolute_error: 3.2833\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 3.18406\n",
            "Epoch 473/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9939 - mean_absolute_error: 2.9939 - val_loss: 3.2931 - val_mean_absolute_error: 3.2931\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 3.18406\n",
            "Epoch 474/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 2.9895 - mean_absolute_error: 2.9895 - val_loss: 3.2773 - val_mean_absolute_error: 3.2773\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 3.18406\n",
            "Epoch 475/500\n",
            "1376/1376 [==============================] - 0s 77us/step - loss: 2.9770 - mean_absolute_error: 2.9770 - val_loss: 3.2876 - val_mean_absolute_error: 3.2876\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 3.18406\n",
            "Epoch 476/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9730 - mean_absolute_error: 2.9730 - val_loss: 3.2433 - val_mean_absolute_error: 3.2433\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 3.18406\n",
            "Epoch 477/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9780 - mean_absolute_error: 2.9780 - val_loss: 3.2539 - val_mean_absolute_error: 3.2539\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 3.18406\n",
            "Epoch 478/500\n",
            "1376/1376 [==============================] - 0s 79us/step - loss: 2.9802 - mean_absolute_error: 2.9802 - val_loss: 3.2757 - val_mean_absolute_error: 3.2757\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 3.18406\n",
            "Epoch 479/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9827 - mean_absolute_error: 2.9827 - val_loss: 3.2120 - val_mean_absolute_error: 3.2120\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 3.18406\n",
            "Epoch 480/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9882 - mean_absolute_error: 2.9882 - val_loss: 3.2548 - val_mean_absolute_error: 3.2548\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 3.18406\n",
            "Epoch 481/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9715 - mean_absolute_error: 2.9715 - val_loss: 3.2310 - val_mean_absolute_error: 3.2310\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 3.18406\n",
            "Epoch 482/500\n",
            "1376/1376 [==============================] - 0s 73us/step - loss: 2.9593 - mean_absolute_error: 2.9593 - val_loss: 3.2217 - val_mean_absolute_error: 3.2217\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 3.18406\n",
            "Epoch 483/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 2.9739 - mean_absolute_error: 2.9739 - val_loss: 3.2954 - val_mean_absolute_error: 3.2954\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 3.18406\n",
            "Epoch 484/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9390 - mean_absolute_error: 2.9390 - val_loss: 3.2870 - val_mean_absolute_error: 3.2870\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 3.18406\n",
            "Epoch 485/500\n",
            "1376/1376 [==============================] - 0s 75us/step - loss: 3.0142 - mean_absolute_error: 3.0142 - val_loss: 3.3500 - val_mean_absolute_error: 3.3500\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 3.18406\n",
            "Epoch 486/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9846 - mean_absolute_error: 2.9846 - val_loss: 3.2203 - val_mean_absolute_error: 3.2203\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 3.18406\n",
            "Epoch 487/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9759 - mean_absolute_error: 2.9759 - val_loss: 3.2920 - val_mean_absolute_error: 3.2920\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 3.18406\n",
            "Epoch 488/500\n",
            "1376/1376 [==============================] - 0s 81us/step - loss: 2.9526 - mean_absolute_error: 2.9526 - val_loss: 3.2469 - val_mean_absolute_error: 3.2469\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 3.18406\n",
            "Epoch 489/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9867 - mean_absolute_error: 2.9867 - val_loss: 3.1971 - val_mean_absolute_error: 3.1971\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 3.18406\n",
            "Epoch 490/500\n",
            "1376/1376 [==============================] - 0s 83us/step - loss: 2.9544 - mean_absolute_error: 2.9544 - val_loss: 3.1948 - val_mean_absolute_error: 3.1948\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 3.18406\n",
            "Epoch 491/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9696 - mean_absolute_error: 2.9696 - val_loss: 3.2796 - val_mean_absolute_error: 3.2796\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 3.18406\n",
            "Epoch 492/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9803 - mean_absolute_error: 2.9803 - val_loss: 3.2178 - val_mean_absolute_error: 3.2178\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 3.18406\n",
            "Epoch 493/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9983 - mean_absolute_error: 2.9983 - val_loss: 3.3895 - val_mean_absolute_error: 3.3895\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 3.18406\n",
            "Epoch 494/500\n",
            "1376/1376 [==============================] - 0s 76us/step - loss: 2.9873 - mean_absolute_error: 2.9873 - val_loss: 3.2198 - val_mean_absolute_error: 3.2198\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 3.18406\n",
            "Epoch 495/500\n",
            "1376/1376 [==============================] - 0s 74us/step - loss: 2.9865 - mean_absolute_error: 2.9865 - val_loss: 3.4405 - val_mean_absolute_error: 3.4405\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 3.18406\n",
            "Epoch 496/500\n",
            "1376/1376 [==============================] - 0s 70us/step - loss: 3.1419 - mean_absolute_error: 3.1419 - val_loss: 3.5357 - val_mean_absolute_error: 3.5357\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 3.18406\n",
            "Epoch 497/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 3.0860 - mean_absolute_error: 3.0860 - val_loss: 3.4181 - val_mean_absolute_error: 3.4181\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 3.18406\n",
            "Epoch 498/500\n",
            "1376/1376 [==============================] - 0s 80us/step - loss: 3.0775 - mean_absolute_error: 3.0775 - val_loss: 3.2697 - val_mean_absolute_error: 3.2697\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 3.18406\n",
            "Epoch 499/500\n",
            "1376/1376 [==============================] - 0s 72us/step - loss: 2.9914 - mean_absolute_error: 2.9914 - val_loss: 3.3505 - val_mean_absolute_error: 3.3505\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 3.18406\n",
            "Epoch 500/500\n",
            "1376/1376 [==============================] - 0s 71us/step - loss: 2.9702 - mean_absolute_error: 2.9702 - val_loss: 3.2602 - val_mean_absolute_error: 3.2602\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 3.18406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1cef4e1ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbdDCieXidvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_file=\"Weights-435--3.18406.hdf5\"\n",
        "CNN_model.load_weights(weights_file)\n",
        "CNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwNl6IjFlgWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6c2c8b34-135f-4e59-bf47-c21b34b38818"
      },
      "source": [
        "test.columns"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ishol/week', 'name', 'quantity', 'unit_cogs', 'monthly_Avgtemp',\n",
              "       'monthly_avg_FeelsLikeC', 'monthly_avg_HeatIndexC',\n",
              "       'monthly_avg_cloudcover', 'monthly_avg_humidity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCxt02A_l5ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=test.drop(['name'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZDb86wslajp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_test=test['quantity']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuylqKdwpW51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=test.drop(['quantity'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHh4zvJ5l4rW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8454221-ec07-41d0-a838-efc2100ef449"
      },
      "source": [
        "import numpy as np\n",
        "print(\"[INFO] predicting quantities...\")\n",
        "preds = CNN_model.predict(test)\n",
        "# compute the difference between the *predicted* quantities and the\n",
        "# *actual* quantities, then compute the percentage difference and\n",
        "# the absolute percentage difference\n",
        "diff = preds.flatten() - target_test\n",
        "percentDiff = (diff / target_test) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] predicting quantities...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxHM0qDysilE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "8902bd90-280a-4068-88dc-ca09ca765e53"
      },
      "source": [
        "print(absPercentDiff)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "740       96.342397\n",
            "1246       1.163733\n",
            "798       34.994829\n",
            "1454      30.498356\n",
            "89        88.081824\n",
            "           ...     \n",
            "149     2219.602776\n",
            "1023      58.030379\n",
            "488       69.423103\n",
            "626        1.228178\n",
            "1132      92.203295\n",
            "Name: quantity, Length: 431, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}